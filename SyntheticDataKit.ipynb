{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbEgnk9IikZ2dxoZ/aFeX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linux-Server/AI_Engineering/blob/main/SyntheticDataKit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference link : https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Meta_Synthetic_Data_Llama3_2_(3B).ipynb#scrollTo=VrBwG2KT7dam\n",
        "\n"
      ],
      "metadata": {
        "id": "25SXvhubHzrM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EMyzrEgW3yNr"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "    !uv pip install synthetic-data-kit==0.0.3\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except: get_numpy = \"numpy\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade \\\n",
        "        unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "!uv pip install transformers==4.55.4\n",
        "!uv pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "njKlmazE7VXR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.dataprep import SyntheticDataKit\n",
        "\n",
        "checkpoint = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "generator = SyntheticDataKit(model_name=checkpoint, max_seq_length=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hmq77mL4njD",
        "outputId": "b951a6a3-7a4c-4aa7-8891-d9b6cac18cec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "INFO 10-01 13:21:44 [__init__.py:244] Automatically detected platform cuda.\n",
            "ERROR 10-01 13:21:45 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 10-01 13:21:52 [vllm_utils.py:689] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 10-01 13:21:52 [vllm_utils.py:717] Unsloth: Patching vLLM v0 graph capture\n",
            "Unsloth: Using dtype = torch.float16 for vLLM.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 97.33%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 224.\n",
            "Unsloth: vLLM's KV Cache can use up to 8.36 GB. Also swap space = 0 GB.\n",
            "vLLM STDOUT: INFO 10-01 13:22:01 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 10-01 13:22:05 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "vLLM STDOUT: INFO 10-01 13:22:05 [cli_args.py:325] non-default args: {'model': 'unsloth/Llama-3.2-3B-Instruct', 'dtype': 'float16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'gpu_memory_utilization': 0.9733171028606208, 'swap_space': 0.0, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 224, 'enable_chunked_prefill': True, 'disable_log_stats': True}\n",
            "vLLM STDOUT: INFO 10-01 13:22:18 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
            "vLLM STDOUT: WARNING 10-01 13:22:18 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "vLLM STDOUT: INFO 10-01 13:22:18 [config.py:1472] Using max model len 2048\n",
            "vLLM STDOUT: WARNING 10-01 13:22:18 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "vLLM STDOUT: INFO 10-01 13:22:20 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "vLLM STDOUT: INFO 10-01 13:22:20 [api_server.py:268] Started engine process with PID 5540\n",
            "vLLM STDOUT: INFO 10-01 13:22:30 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 10-01 13:22:33 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":224,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
            "vLLM STDOUT: INFO 10-01 13:22:34 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "vLLM STDOUT: INFO 10-01 13:22:34 [cuda.py:360] Using XFormers backend.\n",
            "vLLM STDOUT: INFO 10-01 13:22:35 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "vLLM STDOUT: INFO 10-01 13:22:35 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
            "vLLM STDOUT: INFO 10-01 13:22:36 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "vLLM STDOUT: INFO 10-01 13:23:04 [default_loader.py:272] Loading weights took 28.03 seconds\n",
            "vLLM STDOUT: INFO 10-01 13:23:05 [model_runner.py:1203] Model loading took 6.0160 GiB and 28.776337 seconds\n",
            "vLLM STDOUT: INFO 10-01 13:23:07 [worker.py:294] Memory profiling takes 1.58 seconds\n",
            "vLLM STDOUT: INFO 10-01 13:23:07 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.97) = 14.35GiB\n",
            "vLLM STDOUT: INFO 10-01 13:23:07 [worker.py:294] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.04GiB; the rest of the memory reserved for KV Cache is 7.24GiB.\n",
            "vLLM STDOUT: INFO 10-01 13:23:07 [executor_base.py:113] # cuda blocks: 4236, # CPU blocks: 0\n",
            "vLLM STDOUT: INFO 10-01 13:23:07 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 33.09x\n",
            "vLLM STDOUT: INFO 10-01 13:23:07 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "vLLM STDOUT: INFO 10-01 13:23:36 [model_runner.py:1671] Graph capturing finished in 29 secs, took 0.17 GiB\n",
            "vLLM STDOUT: INFO 10-01 13:23:36 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 31.80 seconds\n",
            "vLLM STDOUT: WARNING 10-01 13:23:37 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:29] Available routes are:\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /docs, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /health, Methods: GET\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /load, Methods: GET\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /ping, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /ping, Methods: GET\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /tokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /detokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/models, Methods: GET\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /version, Methods: GET\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /classify, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /score, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "vLLM STDOUT: INFO 10-01 13:23:37 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "vLLM Server Ready Detected\n",
            "vLLM STDOUT: INFO:     127.0.0.1:50428 - \"GET /metrics HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.prepare_qa_generation(output_folder=\"data\", temperature=0.7, top_p=0.95, overlap=64, max_generation_tokens=512)"
      ],
      "metadata": {
        "id": "CgN9o2R75Pqr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit system-check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkSjQ9p--O_u",
        "outputId": "62bf11ec-afd3-44b3-b760-5f684ef9db52"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:39196 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1759325716\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-f45ab093bfb24898b5b764dc46081f8f'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1759325716\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m⠋\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        ingest \"https://www.alongdustyroads.com/posts/kindness-of-kerala\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayzB4SyM-pjy",
        "outputId": "3b0654da-eae8-4855-ebad-28677000d859"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K\u001b[32m⠦\u001b[0m Processing https://www.alongdustyroads.com/posts/kindness-of-kerala...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/www_alongdustyroads_com.txt\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Truncate document\n",
        "filenames = generator.chunk_data(\"data/output/www_alongdustyroads_com.txt\")\n",
        "print(len(filenames), filenames[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUq1tUfqBMCb",
        "outputId": "7ae61e7a-55ed-4da2-a97e-5a8634e47373"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 ['data/output/www_alongdustyroads_com_0.txt', 'data/output/www_alongdustyroads_com_1.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Process 3 chunks for now -> can increase but slower!\n",
        "for filename in filenames[:3]:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {filename} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjksHTaXBr_T",
        "outputId": "1cb0f1cc-12fe-480d-f908-12f90f091b7f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:39260 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:39262 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_0.txt...vLLM STDOUT: INFO 10-01 13:56:55 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "vLLM STDOUT: INFO 10-01 13:56:55 [logger.py:43] Received request chatcmpl-01ab07d4273047cc8ff2e1fb0e17c5da: prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 01 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThe Kindness of Kerala — ALONG DUSTY ROADS\\n0\\nSkip to Content\\nHome\\nAbout\\nBlog\\nPlaces\\nEverywhere\\nEurope\\nCentral America & Mexico\\nSouth America\\nNorth America & Caribbean\\nAsia\\nAfrica\\nALONG DUSTY ROADS\\nOpen Menu\\nClose Menu\\nHome\\nAbout\\nBlog\\nPlaces\\nEverywhere\\nEurope\\nCentral America & Mexico\\nSouth America\\nNorth America & Caribbean\\nAsia\\nAfrica\\nALONG DUSTY ROADS\\nOpen Menu\\nClose Menu\\nHome\\nAbout\\nBlog\\nFolder:\\nPlaces\\nBack\\nEverywhere\\nEurope\\nCentral America & Mexico\\nSouth America\\nNorth America & Caribbean\\nAsia\\nAfrica\\nThe Kindness of Kerala\\nindiatravel journal\\n24 Nov\\nWritten By Andrew\\nIt was the sort of place people dream about.Soft waves lapped at our feet. The gentle breeze whispered through the palm tree fronds. Strange birds sang a different tune. A beach of bruised gold, hidden away down a dusty road, was ours and ours alone.We had arrived in Kannur the previous evening, late and under cover of darkness. A much-delayed train journey from Fort Kochi and an auto-rickshaw ride in the moonlight toward the sound of the waves brought us to our guesthouse by the coast. The owner, Bipin, a man with warm-hearted eyes and a gentle manner, had spent a number of years working in Dubai but returned to set up this business where he grew up. We were his only guests, and over several nights he would cook us traditional meals of subtle spice with ingredients from his garden, and share his wisdom as well as his table.This part of northern Kerala is often overlooked by travellers on their way up to Goa. For those going in the other direction, a rush toward the better-known cliff-sides of Varkala, the houseboats of Alleppey, or the tea-stained hills and swirling mists of Munnar is usually prioritised.With time on our hands though, the opportunity to go somewhere a little-less visited in this tropical state of 35 million by the Arabian Sea had obvious appeal.The only thing is, we didn't come to Kerala for paradise.Yes, we had happily stumbled upon it mere steps from our bedroom door, but this trip to escape the long, dark British winter was never just about seeking out splendid sun-kissed isolation (thank god for that given the storm that lay ahead in 2020).This trip was about kindness\\nWithin my Scottish family, Kerala has an unbreakable connection with that single word.My sister - the person whose travels inspired many of my own - suffered a serious accident in Dubai nearly two decades ago, resulting in weeks confined to a hospital bed in a foreign country.That's something that no traveller and no family wants to deal with. My mother flew out to be by her side, a familiar face in somewhere unfamiliar.The majority, if not all, the nurses who cared for her there hailed from Kerala. As with Bipin, the education and dedication of Keralans is a sought after commodity in the Middle East, and it is not unusual to meet a number of people on your travels who have worked or have family who continue to live there. The nurses' attentiveness, gentleness, care, and kindness throughout her stay left an indelible impression on both my mother and sister (who I can happily report made a full recovery), and was often brought up at family gatherings (in between those sibling arguments which we were thankfully able to pick right back up from where we left them).And so, when I said that Emily and I had decided to go to Kerala for our big backpacking trip of 2019, and first taste of India, those nurses and their kindness was the first thing mentioned by both. “The nicest people in the world,” my sister said.This concerned me.It takes a lifetime to build a good reputation, but only a moment to ruin it.We've visited many Latin American cities that feature on lists of 'the world's most dangerous'; places where the average traveller needs only a few days to challenge the preconceived notion of danger and confirm that most people living there really aren't dangerous at all.However, what if the prevailing stereotype before you go somewhere new is one of virtue, rather than vice?One where kindness and human connection underpin the experience? Isn't that easier to prove wrong through one unfortunate incident, one conversation gone awry, or ill-tempered exchange with the wrong person at the wrong time?\\nBack in Kannur, Bipin arranged for a rickshaw to bring us to a couple of places we were curious to visit. The first was a beedi cooperative, the second<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-01 13:56:55 [engine.py:317] Added request chatcmpl-01ab07d4273047cc8ff2e1fb0e17c5da.\n",
            "\u001b[2K\u001b[32m⠇\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_0.txt...vLLM STDOUT: INFO:     127.0.0.1:39270 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-01 13:57:13 [logger.py:43] Received request chatcmpl-658b3616e98b4dfd9408eaa3baf9a300: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 01 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nThe Kindness of Kerala — ALONG DUSTY ROADS\\n0\\nSkip to Content\\nHome\\nAbout\\nBlog\\nPlaces\\nEverywhere\\nEurope\\nCentral America & Mexico\\nSouth America\\nNorth America & Caribbean\\nAsia\\nAfrica\\nALONG DUSTY ROADS\\nOpen Menu\\nClose Menu\\nHome\\nAbout\\nBlog\\nPlaces\\nEverywhere\\nEurope\\nCentral America & Mexico\\nSouth America\\nNorth America & Caribbean\\nAsia\\nAfrica\\nALONG DUSTY ROADS\\nOpen Menu\\nClose Menu\\nHome\\nAbout\\nBlog\\nFolder:\\nPlaces\\nBack\\nEverywhere\\nEurope\\nCentral America & Mexico\\nSouth America\\nNorth America & Caribbean\\nAsia\\nAfrica\\nThe Kindness of Kerala\\nindiatravel journal\\n24 Nov\\nWritten By Andrew\\nIt was the sort of place people dream about.Soft waves lapped at our feet. The gentle breeze whispered through the palm tree fronds. Strange birds sang a different tune. A beach of bruised gold, hidden away down a dusty road, was ours and ours alone.We had arrived in Kannur the previous evening, late and under cover of darkness. A much-delayed train journey from Fort Kochi and an auto-rickshaw ride in the moonlight toward the sound of the waves brought us to our guesthouse by the coast. The owner, Bipin, a man with warm-hearted eyes and a gentle manner, had spent a number of years working in Dubai but returned to set up this business where he grew up. We were his only guests, and over several nights he would cook us traditional meals of subtle spice with ingredients from his garden, and share his wisdom as well as his table.This part of northern Kerala is often overlooked by travellers on their way up to Goa. For those going in the other direction, a rush toward the better-known cliff-sides of Varkala, the houseboats of Alleppey, or the tea-stained hills and swirling mists of Munnar is usually prioritised.With time on our hands though, the opportunity to go somewhere a little-less visited in this tropical state of 35 million by the Arabian Sea had obvious appeal.The only thing is, we didn\\'t come to Kerala for paradise.Yes, we had happily stumbled upon it mere steps from our bedroom door, but this trip to escape the long, dark British winter was never just about seeking out splendid sun-kissed isolation (thank god for that given the storm that lay ahead in 2020).This trip was about kindness\\nWithin my Scottish family, Kerala has an unbreakable connection with that single word.My sister - the person whose travels inspired many of my own - suffered a serious accident in Dubai nearly two decades ago, resulting in weeks confined to a hospital bed in a foreign country.That\\'s something that no traveller and no family wants to deal with. My mother flew out to be by her side, a familiar face in somewhere unfamiliar.The majority, if not all, the nurses who cared for her there hailed from Kerala. As with Bipin, the education and dedication of Keralans is a sought after commodity in the Middle East, and it is not unusual to meet a number of people on your travels who have worked or have family who continue to live there. The nurses\\' attentiveness, gentleness, care, and kindness throughout her stay left an indelible impression on both my mother and sister (who I can happily report made a full recovery), and was often brought up at family gatherings (in between those sibling arguments which we were thankfully able to pick right back up from where we left them).And so, when I said that Emily and I had decided to go to Kerala for our big backpacking trip of 2019, and first taste of India, those nurses and their kindness was the first thing mentioned by both. “The nicest people in the world,” my sister said.This concerned me.It takes a lifetime to build a good reputation, but only a moment to ruin it.We\\'ve visited many Latin American cities that feature on lists of \\'the world\\'s most dangerous\\'; places where the average traveller needs only a few days to challenge the preconceived notion of danger and confirm that most people living there really aren\\'t dangerous at all.However, what if the prevailing stereotype before you go somewhere new is one of virtue, rather than vice?One where kindness and human connection underpin the experience? Isn\\'t that easier to prove wrong through one unfortunate incident, one conversation gone awry, or ill-tempered exchange with the wrong person at the wrong time?\\nBack in Kannur, Bipin arranged for a rickshaw to bring us to a couple of places we were curious to visit. The first was a beedi cooperative, the second<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-01 13:57:13 [engine.py:317] Added request chatcmpl-658b3616e98b4dfd9408eaa3baf9a300.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠴\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_0.txt...vLLM STDOUT: INFO:     127.0.0.1:54270 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/www_alongdustyroads_com_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to \n",
            "data/generated/www_alongdustyroads_com_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠧\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/www_alongdustyroads_com_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40790 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40796 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-01 13:57:36 [logger.py:43] Received request chatcmpl-ce3c21b07b5342c69550042fc2886c89: prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 01 Oct 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nwrong through one unfortunate incident, one conversation gone awry, or ill-tempered exchange with the wrong person at the wrong time?\\nBack in Kannur, Bipin arranged for a rickshaw to bring us to a couple of places we were curious to visit. The first was a beedi cooperative, the second a textile factory; both were linked by the fact that the human hand was still responsible for much of the work, the craft. We arrived unannounced to both, which aren't actually tourist attractions, but were welcomed without suspicion.The women who cut, snip, roll, and pile the old cigarettes laughed and mocked gently, babbling away about why we weren't yet married. The men of the textile factory, hard at work in the shafts of golden light, took the time to smile and say hello even though it wasn't required.It felt like this was a welcome we wouldn’t actually be able to outstay.Between the two, we sought out a chai on a nondescript road with a nondescript number of little stalls, opposite a seemingly half-abandoned shopping centre. Even though we found the ten-rupee tea served far too sweet for our tastebuds, our little chai stops had become an important ritual for us in Kerala. Aside from the refreshment, there was usually someone to chat to, some people-watching opportunities, or photogenic possibilities as the brew was ceremonially poured from upon high and the saccharine steam would rise from the glass cups below.On this day, we were joined within the small space of peeling bright blue walls by a holy man, three street cleaners, and a jovial old boy who could have been anything from 70 to 100 years old.\\nThe holy man in orange - who appeared like a hallucination from a dark corner - gave us two small black-spotted bananas and a blessing before silently departing on foot. The street cleaners, two women and men, shared jokes whilst eating their lunch and answering our questions about nothing remarkable; the man offered to buy our teas (we insisted he didn't have to, and are still not sure if this was the culturally right or wrong thing to do for him in such a scenario). The old boy made everyone laugh in a language we couldn't understand, possibly at our expense, but in a way which brought the room together over half an hour rather than made us feel apart.Steam continued to rise as we sipped from our glasses of sugar mountain sweet tea as hospitality, rather than hostility, confirmed itself as the norm wherever we ventured in Kerala.After three weeks of travel here, it's a testament to its people that I was able to return to my family with more tales of kindness, rather than anecdotes opposing it, from this corner of southern India.\\ufeff\\nThis article was brought to you in partnership with Kerala Tourism, but based on our own experiences, memories, and thoughts.For more, take a look at the video below, or read our Kerala travel guides.\\nPlan Your Keralan Adventure With Our Guides\\nFeatured\\nOur Favourite Things to Do in Kochi | The Pretty Port of Kerala\\n7 Things to Do in Munnar | The Tea Capital of Kerala\\nThe Kindness of Kerala\\nWhat Do Things Cost in Kerala?\\n23 Things to Know Before You Visit Kerala\\nOur Guide to Kannur | The Most Beautiful Beach in Kerala\\nHow To Apply for the India E-Visa | A Guide for Travellers\\nA Traveller's Guide to Varkala | Kerala's Backpacker Beach Town\\nThe Kerala Backwaters | 11 Things To Know Before You Visit\\nkerala\\nAndrew\\nPrevious\\nPrevious\\nWhat Things Cost in Venice | Plan Your City Break Budget\\nNext\\nNext\\nA Beginner's Guide to Couchsurfing\\nfollow us on instagram\\nAlong Dusty Roads is a travel blog by the British couple, Andrew & Emily. It promotes sustainable and responsible travel for the curious modern day explorer, and is 100% independent.· buy us a coffee ·\\nDestinationsColombiaItalyJordanPeruCambodiaEVERYWHERE\\nThe Newsletter\\nEmail Address\\nSign Up\\nThanks! Check your inbox to confirm.\\nPlease note that some links on our blog are affiliate partners.If you choose to purchase through these links, we may receive a small commission at no extra cost to you. By using them, you are directly supporting Along Dusty Roads to remain an independent travel blog, and to create free guides to help you travel more, travel better.If you’ve really enjoyed our guides, you can buy us a ‘virtual’ coffee here. Please don’t steal our words or images.\\n© Along Dusty Roads Ltd 2014-2025\\nHome\\xa0\\xa0\\xa0 Work With Us\\xa0\\xa0\\xa0 Disclaimer and Privacy Policy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-01 13:57:36 [engine.py:317] Added request chatcmpl-ce3c21b07b5342c69550042fc2886c89.\n",
            "\u001b[2K\u001b[32m⠋\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_1.txt...vLLM STDOUT: INFO:     127.0.0.1:40802 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 10-01 13:57:41 [logger.py:43] Received request chatcmpl-a14b2f7496f04a46867bf33a17e4c58d: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 01 Oct 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n wrong through one unfortunate incident, one conversation gone awry, or ill-tempered exchange with the wrong person at the wrong time?\\nBack in Kannur, Bipin arranged for a rickshaw to bring us to a couple of places we were curious to visit. The first was a beedi cooperative, the second a textile factory; both were linked by the fact that the human hand was still responsible for much of the work, the craft. We arrived unannounced to both, which aren\\'t actually tourist attractions, but were welcomed without suspicion.The women who cut, snip, roll, and pile the old cigarettes laughed and mocked gently, babbling away about why we weren\\'t yet married. The men of the textile factory, hard at work in the shafts of golden light, took the time to smile and say hello even though it wasn\\'t required.It felt like this was a welcome we wouldn’t actually be able to outstay.Between the two, we sought out a chai on a nondescript road with a nondescript number of little stalls, opposite a seemingly half-abandoned shopping centre. Even though we found the ten-rupee tea served far too sweet for our tastebuds, our little chai stops had become an important ritual for us in Kerala. Aside from the refreshment, there was usually someone to chat to, some people-watching opportunities, or photogenic possibilities as the brew was ceremonially poured from upon high and the saccharine steam would rise from the glass cups below.On this day, we were joined within the small space of peeling bright blue walls by a holy man, three street cleaners, and a jovial old boy who could have been anything from 70 to 100 years old.\\nThe holy man in orange - who appeared like a hallucination from a dark corner - gave us two small black-spotted bananas and a blessing before silently departing on foot. The street cleaners, two women and men, shared jokes whilst eating their lunch and answering our questions about nothing remarkable; the man offered to buy our teas (we insisted he didn\\'t have to, and are still not sure if this was the culturally right or wrong thing to do for him in such a scenario). The old boy made everyone laugh in a language we couldn\\'t understand, possibly at our expense, but in a way which brought the room together over half an hour rather than made us feel apart.Steam continued to rise as we sipped from our glasses of sugar mountain sweet tea as hospitality, rather than hostility, confirmed itself as the norm wherever we ventured in Kerala.After three weeks of travel here, it\\'s a testament to its people that I was able to return to my family with more tales of kindness, rather than anecdotes opposing it, from this corner of southern India.\\ufeff\\nThis article was brought to you in partnership with Kerala Tourism, but based on our own experiences, memories, and thoughts.For more, take a look at the video below, or read our Kerala travel guides.\\nPlan Your Keralan Adventure With Our Guides\\nFeatured\\nOur Favourite Things to Do in Kochi | The Pretty Port of Kerala\\n7 Things to Do in Munnar | The Tea Capital of Kerala\\nThe Kindness of Kerala\\nWhat Do Things Cost in Kerala?\\n23 Things to Know Before You Visit Kerala\\nOur Guide to Kannur | The Most Beautiful Beach in Kerala\\nHow To Apply for the India E-Visa | A Guide for Travellers\\nA Traveller\\'s Guide to Varkala | Kerala\\'s Backpacker Beach Town\\nThe Kerala Backwaters | 11 Things To Know Before You Visit\\nkerala\\nAndrew\\nPrevious\\nPrevious\\nWhat Things Cost in Venice | Plan Your City Break Budget\\nNext\\nNext\\nA Beginner\\'s Guide to Couchsurfing\\nfollow us on instagram\\nAlong Dusty Roads is a travel blog by the British couple, Andrew & Emily. It promotes sustainable and responsible travel for the curious modern day explorer, and is 100% independent.· buy us a coffee ·\\nDestinationsColombiaItalyJordanPeruCambodiaEVERYWHERE\\nThe Newsletter\\nEmail Address\\nSign Up\\nThanks! Check your inbox to confirm.\\nPlease note that some links on our blog are affiliate partners.If you choose to purchase through these links, we may receive a small commission at no extra cost to you. By using them, you are directly supporting Along Dusty Roads to remain an independent travel blog, and to create free guides to help you travel more, travel better.If you’ve really enjoyed our guides, you can buy us a ‘virtual’ coffee here. Please don’t steal our words or images.\\n© Along Dusty Roads Ltd 2014-2025\\nHome\\xa0\\xa0\\xa0 Work With Us\\xa0\\xa0\\xa0 Disclaimer and Privacy Policy<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 10-01 13:57:41 [engine.py:317] Added request chatcmpl-a14b2f7496f04a46867bf33a17e4c58d.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m⠸\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_1.txt...vLLM STDOUT: INFO:     127.0.0.1:33830 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 14 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/www_alongdustyroads_com_1_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to \n",
            "data/generated/www_alongdustyroads_com_1_qa_pairs.json\n",
            "\u001b[2K\u001b[32m⠦\u001b[0m Generating qa content from data/output/www_alongdustyroads_com_1.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/www_alongdustyroads_com_1_qa_pairs.json\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pairs_filenames = [\n",
        "    f\"data/generated/www_alongdustyroads_com_{i}_qa_pairs.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbBs_DG3B-0W",
        "outputId": "6d85cd58-3497-4a7a-805f-1da76276916d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/www_alongdustyroads_com_0_qa_pairs.json to ft format\n",
            "with json storage...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n",
            "\u001b[1;32mdata/final/www_alongdustyroads_com_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m⠋\u001b[0m Converting data/generated/www_alongdustyroads_com_1_qa_pairs.json to ft format\n",
            "with json storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\n",
            "\u001b[1;32mdata/final/www_alongdustyroads_com_1_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "final_filenames = [\n",
        "    f\"data/final/www_alongdustyroads_com_{i}_qa_pairs_ft.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "conversations = pd.concat([\n",
        "    pd.read_json(name) for name in final_filenames\n",
        "]).reset_index(drop = True)\n",
        "\n",
        "dataset = Dataset.from_pandas(conversations)"
      ],
      "metadata": {
        "id": "AYQf9xcVEfJ7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijC6-GOEEwJI",
        "outputId": "a1c1d3b5-3ad0-4cd3-ae4a-16cec1666ac8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages'],\n",
              "    num_rows: 27\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[21]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFvG0YJPExNa",
        "outputId": "0e7359d4-154f-4a5f-c5af-33d7f710d4bd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'What made the chai stops an important ritual for the author and Andrew?',\n",
              "   'role': 'user'},\n",
              "  {'content': 'the refreshment, chat, people-watching opportunities, or photogenic possibilities',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.cleanup()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AF0Mq2QFCQP",
        "outputId": "c4b58228-1d3a-4a9e-b875-733a3d2bcd68"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to terminate the VLLM server gracefully...\n",
            "vLLM STDOUT: INFO 10-01 14:04:21 [launcher.py:80] Shutting down FastAPI HTTP server.\n",
            "Server terminated gracefully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rJDn8JdqFqEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}